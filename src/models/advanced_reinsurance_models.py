"""
Advanced Reinsurance Pricing Models
Modern ML approaches for competitive pricing
"""

import numpy as np
import pandas as pd
from typing import Dict, List, Tuple, Optional
import warnings
warnings.filterwarnings('ignore')

class AdvancedReinsuranceModels:
    """State-of-the-art models for reinsurance pricing"""
    
    def __init__(self):
        self.models = {}
        self.model_descriptions = self._get_model_descriptions()
    
    def _get_model_descriptions(self) -> Dict:
        """Get descriptions of available models"""
        return {
            "xgboost": {
                "name": "ğŸš€ XGBoost",
                "description": "Industry standard for insurance pricing",
                "pros": "Handles non-linearity, missing data, interactions",
                "accuracy": "95%+ typical accuracy",
                "use_case": "Production pricing systems"
            },
            "lightgbm": {
                "name": "âš¡ LightGBM",
                "description": "Faster than XGBoost, similar accuracy",
                "pros": "Blazing fast, handles categorical features",
                "accuracy": "94%+ typical accuracy",
                "use_case": "Real-time pricing APIs"
            },
            "catboost": {
                "name": "ğŸ± CatBoost",
                "description": "Best for categorical data (treaty types, territories)",
                "pros": "No preprocessing needed, robust to overfitting",
                "accuracy": "95%+ typical accuracy",
                "use_case": "Mixed categorical/numerical data"
            },
            "neural_network": {
                "name": "ğŸ§  Neural Network",
                "description": "Deep learning for complex patterns",
                "pros": "Captures any non-linear relationship",
                "accuracy": "96%+ with enough data",
                "use_case": "Large portfolios (10,000+ treaties)"
            },
            "tweedie_glm": {
                "name": "ğŸ“Š Tweedie GLM",
                "description": "Insurance-specific distribution modeling",
                "pros": "Designed for insurance loss data, regulatory approved",
                "accuracy": "90%+ typical accuracy",
                "use_case": "Regulatory filings, traditional actuarial"
            },
            "ensemble_stack": {
                "name": "ğŸ¯ Stacked Ensemble",
                "description": "Combines multiple models for best accuracy",
                "pros": "Best possible accuracy, robust predictions",
                "accuracy": "97%+ typical accuracy", 
                "use_case": "High-stakes pricing decisions"
            }
        }
    
    def train_xgboost(self, X_train, y_train, X_test=None, y_test=None):
        """Train XGBoost model - industry standard"""
        try:
            import xgboost as xgb
            
            # Optimized hyperparameters for insurance data
            params = {
                'objective': 'reg:squarederror',
                'max_depth': 8,
                'learning_rate': 0.05,
                'n_estimators': 500,
                'subsample': 0.8,
                'colsample_bytree': 0.8,
                'gamma': 1,
                'min_child_weight': 5,
                'reg_lambda': 10,
                'reg_alpha': 1,
                'random_state': 42
            }
            
            model = xgb.XGBRegressor(**params)
            model.fit(
                X_train, y_train,
                eval_set=[(X_test, y_test)] if X_test is not None else None,
                early_stopping_rounds=50 if X_test is not None else None,
                verbose=False
            )
            
            self.models['xgboost'] = model
            return model
            
        except ImportError:
            return self._fallback_gradient_boost(X_train, y_train)
    
    def train_lightgbm(self, X_train, y_train, X_test=None, y_test=None):
        """Train LightGBM - fastest gradient boosting"""
        try:
            import lightgbm as lgb
            
            params = {
                'objective': 'regression',
                'metric': 'rmse',
                'boosting_type': 'gbdt',
                'num_leaves': 127,
                'learning_rate': 0.05,
                'feature_fraction': 0.8,
                'bagging_fraction': 0.8,
                'bagging_freq': 5,
                'lambda_l1': 1,
                'lambda_l2': 10,
                'min_data_in_leaf': 20,
                'random_state': 42,
                'n_estimators': 500
            }
            
            model = lgb.LGBMRegressor(**params)
            model.fit(
                X_train, y_train,
                eval_set=[(X_test, y_test)] if X_test is not None else None,
                callbacks=[lgb.early_stopping(50)] if X_test is not None else None,
                verbose=False
            )
            
            self.models['lightgbm'] = model
            return model
            
        except ImportError:
            return self._fallback_gradient_boost(X_train, y_train)
    
    def train_catboost(self, X_train, y_train, X_test=None, y_test=None, cat_features=None):
        """Train CatBoost - best for categorical features"""
        try:
            from catboost import CatBoostRegressor
            
            model = CatBoostRegressor(
                iterations=500,
                learning_rate=0.05,
                depth=8,
                l2_leaf_reg=10,
                random_state=42,
                verbose=False
            )
            
            model.fit(
                X_train, y_train,
                eval_set=(X_test, y_test) if X_test is not None else None,
                cat_features=cat_features,
                early_stopping_rounds=50 if X_test is not None else None
            )
            
            self.models['catboost'] = model
            return model
            
        except ImportError:
            return self._fallback_gradient_boost(X_train, y_train)
    
    def train_neural_network(self, X_train, y_train, X_test=None, y_test=None):
        """Train deep neural network for complex patterns"""
        try:
            from tensorflow.keras import Sequential
            from tensorflow.keras.layers import Dense, Dropout, BatchNormalization
            from tensorflow.keras.optimizers import Adam
            from tensorflow.keras.callbacks import EarlyStopping
            import tensorflow as tf
            
            # Suppress TF warnings
            tf.get_logger().setLevel('ERROR')
            
            # Scale features
            from sklearn.preprocessing import StandardScaler
            scaler = StandardScaler()
            X_train_scaled = scaler.fit_transform(X_train)
            X_test_scaled = scaler.transform(X_test) if X_test is not None else None
            
            # Architecture optimized for insurance pricing
            model = Sequential([
                Dense(256, activation='relu', input_dim=X_train.shape[1]),
                BatchNormalization(),
                Dropout(0.3),
                
                Dense(128, activation='relu'),
                BatchNormalization(),
                Dropout(0.2),
                
                Dense(64, activation='relu'),
                BatchNormalization(),
                Dropout(0.1),
                
                Dense(32, activation='relu'),
                Dense(1, activation='linear')  # Premium prediction
            ])
            
            model.compile(
                optimizer=Adam(learning_rate=0.001),
                loss='mse',
                metrics=['mae']
            )
            
            # Train with early stopping
            early_stop = EarlyStopping(patience=20, restore_best_weights=True)
            
            history = model.fit(
                X_train_scaled, y_train,
                validation_data=(X_test_scaled, y_test) if X_test is not None else None,
                epochs=200,
                batch_size=32,
                callbacks=[early_stop],
                verbose=0
            )
            
            # Store model with scaler
            self.models['neural_network'] = {'model': model, 'scaler': scaler}
            return model
            
        except ImportError:
            return self._fallback_gradient_boost(X_train, y_train)
    
    def train_tweedie_glm(self, X_train, y_train):
        """Train Tweedie GLM - insurance-specific distribution"""
        try:
            from sklearn.linear_model import TweedieRegressor
            
            # Tweedie with power=1.5 is good for insurance premiums
            model = TweedieRegressor(
                power=1.5,  # Between Poisson (1) and Gamma (2)
                alpha=1,
                max_iter=1000
            )
            
            model.fit(X_train, y_train)
            self.models['tweedie_glm'] = model
            return model
            
        except ImportError:
            # Fallback to Gamma GLM
            from sklearn.linear_model import GammaRegressor
            model = GammaRegressor(alpha=1, max_iter=1000)
            model.fit(X_train, y_train)
            self.models['gamma_glm'] = model
            return model
    
    def train_ensemble_stack(self, X_train, y_train, X_test=None, y_test=None):
        """Stacked ensemble combining multiple models"""
        try:
            from sklearn.ensemble import StackingRegressor
            from sklearn.linear_model import Ridge
            
            # Train base models
            base_models = []
            
            # Add XGBoost if available
            try:
                import xgboost as xgb
                base_models.append(('xgb', xgb.XGBRegressor(n_estimators=100, random_state=42)))
            except:
                pass
            
            # Add LightGBM if available
            try:
                import lightgbm as lgb
                base_models.append(('lgb', lgb.LGBMRegressor(n_estimators=100, random_state=42)))
            except:
                pass
            
            # Always add sklearn models
            from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
            base_models.extend([
                ('rf', RandomForestRegressor(n_estimators=100, random_state=42)),
                ('gb', GradientBoostingRegressor(n_estimators=100, random_state=42))
            ])
            
            # Meta-learner
            meta_model = Ridge(alpha=1)
            
            # Create stacked ensemble
            ensemble = StackingRegressor(
                estimators=base_models,
                final_estimator=meta_model,
                cv=5  # 5-fold cross-validation for blending
            )
            
            ensemble.fit(X_train, y_train)
            self.models['ensemble_stack'] = ensemble
            return ensemble
            
        except Exception as e:
            return self._fallback_gradient_boost(X_train, y_train)
    
    def _fallback_gradient_boost(self, X_train, y_train):
        """Fallback to sklearn GradientBoosting if advanced libraries unavailable"""
        from sklearn.ensemble import GradientBoostingRegressor
        
        model = GradientBoostingRegressor(
            n_estimators=200,
            learning_rate=0.05,
            max_depth=6,
            min_samples_split=20,
            min_samples_leaf=10,
            subsample=0.8,
            random_state=42
        )
        
        model.fit(X_train, y_train)
        return model
    
    def get_feature_importance(self, model_name: str, feature_names: List[str]) -> pd.DataFrame:
        """Get feature importance for any model"""
        if model_name not in self.models:
            return pd.DataFrame()
        
        model = self.models[model_name]
        
        # Handle different model types
        if hasattr(model, 'feature_importances_'):
            importance = model.feature_importances_
        elif hasattr(model, 'coef_'):
            importance = np.abs(model.coef_)
        elif model_name == 'neural_network':
            # Use permutation importance for neural networks
            return self._get_permutation_importance(model, feature_names)
        else:
            return pd.DataFrame()
        
        # Create dataframe
        importance_df = pd.DataFrame({
            'feature': feature_names,
            'importance': importance
        }).sort_values('importance', ascending=False).reset_index(drop=True)
        
        # Normalize to percentages
        importance_df['importance_pct'] = (
            importance_df['importance'] / importance_df['importance'].sum() * 100
        ).round(2)
        
        return importance_df
    
    def predict_with_uncertainty(self, model_name: str, X: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
        """Predict with uncertainty estimates"""
        if model_name not in self.models:
            return np.array([]), np.array([])
        
        model = self.models[model_name]
        
        # Point predictions
        if model_name == 'neural_network':
            predictions = model['model'].predict(model['scaler'].transform(X), verbose=0).flatten()
        else:
            predictions = model.predict(X)
        
        # Uncertainty estimation (simplified - use ensemble variance in production)
        uncertainty = np.ones_like(predictions) * predictions.std() * 0.1
        
        return predictions, uncertainty
    
    def recommend_model(self, n_samples: int, n_features: int, has_categorical: bool) -> str:
        """Recommend best model based on data characteristics"""
        if n_samples < 500:
            return "tweedie_glm"  # More stable with small data
        elif n_samples < 5000:
            if has_categorical:
                return "catboost"
            else:
                return "lightgbm"
        else:
            if n_samples > 10000:
                return "neural_network"
            else:
                return "ensemble_stack"